{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# E(n)-Equivariant Steerable CNNs  -  A concrete example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setGPU: Setting GPU to: 1\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import setGPU\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "import inspect\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "import optax\n",
    "from jaxtyping import Array, Float, Int, PyTree, PRNGKeyArray\n",
    "\n",
    "from escnn import gspaces\n",
    "from escnn import nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we build a **Steerable CNN** and try it on MNIST.\n",
    "\n",
    "Let's also use a group a bit larger: we now build a model equivariant to $8$ rotations.\n",
    "We indicate the group of $N$ discrete rotations as $C_N$, i.e. the **cyclic group** of order $N$.\n",
    "In this case, we will use $C_8$.\n",
    "\n",
    "Because the inputs are still gray-scale images, the input type of the model is again a *scalar field*.\n",
    "\n",
    "However, internally we use *regular fields*: this is equivalent to a *group-equivariant convolutional neural network*.\n",
    "\n",
    "Finally, we build *invariant* features for the final classification task by pooling over the group using *Group Pooling*.\n",
    "\n",
    "The final classification is performed by a two fully connected layers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model\n",
    "\n",
    "Here is the definition of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C8SteerableCNN(nn.EquivariantModule):\n",
    "    layers: list[eqx.Module]\n",
    "    fully_net: list\n",
    "    input_type: nn.FieldType\n",
    "\n",
    "    def __init__(self, key, n_classes=10):\n",
    "        super(C8SteerableCNN, self).__init__()\n",
    "\n",
    "        keys = jax.random.split(key, 8)\n",
    "        \n",
    "        # the model is equivariant under rotations by 45 degrees, modelled by C8\n",
    "        r2_act = gspaces.rot2dOnR2(N=8)\n",
    "        \n",
    "        # the input image is a scalar field, corresponding to the trivial representation\n",
    "        in_type = nn.FieldType(r2_act, [r2_act.trivial_repr])\n",
    "        \n",
    "        # we store the input type for wrapping the images into a geometric tensor during the forward pass\n",
    "        self.input_type = in_type\n",
    "        \n",
    "        # convolution 1\n",
    "        # first specify the output type of the convolutional layer\n",
    "        # we choose 24 feature fields, each transforming under the regular representation of C8\n",
    "        out_type = nn.FieldType(r2_act, 24*[r2_act.regular_repr])\n",
    "        block1 = nn.SequentialModule(\n",
    "            nn.MaskModule(in_type, 29, margin=1),\n",
    "            nn.R2Conv(in_type, out_type, kernel_size=7, padding=1, use_bias=False, key=keys[0]),\n",
    "            nn.InnerBatchNorm(out_type),\n",
    "            nn.ReLU(out_type)\n",
    "        )\n",
    "        \n",
    "        # convolution 2\n",
    "        # the old output type is the input type to the next layer\n",
    "        in_type = block1.out_type\n",
    "        # the output type of the second convolution layer are 48 regular feature fields of C8\n",
    "        out_type = nn.FieldType(r2_act, 48*[r2_act.regular_repr])\n",
    "        block2 = nn.SequentialModule(\n",
    "            nn.R2Conv(in_type, out_type, kernel_size=5, padding=2, use_bias=False, key=keys[1]),\n",
    "            nn.InnerBatchNorm(out_type),\n",
    "            nn.ReLU(out_type)\n",
    "        )\n",
    "        pool1 = nn.PointwiseAvgPoolAntialiased(out_type, sigma=0.66, stride=2)\n",
    "        \n",
    "        # convolution 3\n",
    "        # the old output type is the input type to the next layer\n",
    "        in_type = block2.out_type\n",
    "        # the output type of the third convolution layer are 48 regular feature fields of C8\n",
    "        out_type = nn.FieldType(r2_act, 48*[r2_act.regular_repr])\n",
    "        block3 = nn.SequentialModule(\n",
    "            nn.R2Conv(in_type, out_type, kernel_size=5, padding=2, use_bias=False, key=keys[2]),\n",
    "            nn.InnerBatchNorm(out_type),\n",
    "            nn.ReLU(out_type)\n",
    "        )\n",
    "        \n",
    "        # convolution 4\n",
    "        # the old output type is the input type to the next layer\n",
    "        in_type = block3.out_type\n",
    "        # the output type of the fourth convolution layer are 96 regular feature fields of C8\n",
    "        out_type = nn.FieldType(r2_act, 96*[r2_act.regular_repr])\n",
    "        block4 = nn.SequentialModule(\n",
    "            nn.R2Conv(in_type, out_type, kernel_size=5, padding=2, use_bias=False, key=keys[3]),\n",
    "            nn.InnerBatchNorm(out_type),\n",
    "            nn.ReLU(out_type)\n",
    "        )\n",
    "        pool2 = nn.PointwiseAvgPoolAntialiased(out_type, sigma=0.66, stride=2)\n",
    "        \n",
    "        # convolution 5\n",
    "        # the old output type is the input type to the next layer\n",
    "        in_type = block4.out_type\n",
    "        # the output type of the fifth convolution layer are 96 regular feature fields of C8\n",
    "        out_type = nn.FieldType(r2_act, 96*[r2_act.regular_repr])\n",
    "        block5 = nn.SequentialModule(\n",
    "            nn.R2Conv(in_type, out_type, kernel_size=5, padding=2, use_bias=False, key=keys[4]),\n",
    "            nn.InnerBatchNorm(out_type),\n",
    "            nn.ReLU(out_type)\n",
    "        )\n",
    "        \n",
    "        # convolution 6\n",
    "        # the old output type is the input type to the next layer\n",
    "        in_type = block5.out_type\n",
    "        # the output type of the sixth convolution layer are 64 regular feature fields of C8\n",
    "        out_type = nn.FieldType(r2_act, 64*[r2_act.regular_repr])\n",
    "        block6 = nn.SequentialModule(\n",
    "            nn.R2Conv(in_type, out_type, kernel_size=5, padding=1, use_bias=False, key=keys[5]),\n",
    "            nn.InnerBatchNorm(out_type),\n",
    "            nn.ReLU(out_type)\n",
    "        )\n",
    "        pool3 = nn.PointwiseAvgPoolAntialiased(out_type, sigma=0.66, stride=1, padding=0)\n",
    "        \n",
    "        gpool = nn.GroupPooling(out_type)\n",
    "\n",
    "        self.layers = [block1, block2, pool1, block3, block4, pool2, block5, block6, pool3, gpool]\n",
    "        \n",
    "        # number of output channels\n",
    "        c = gpool.out_type.size\n",
    "        \n",
    "        # Fully Connected\n",
    "        self.fully_net = [\n",
    "            jnp.ravel,\n",
    "            eqx.nn.Linear(c, 64, key=keys[6]),\n",
    "            eqx.nn.BatchNorm(64, axis_name=\"batch\"),\n",
    "            jax.nn.elu,\n",
    "            eqx.nn.Linear(64, n_classes, key=keys[7]),\n",
    "        ]\n",
    "    \n",
    "    def __call__(self, input: Array, state: eqx.nn.State):\n",
    "        # wrap the input tensor in a GeometricTensor\n",
    "        # (associate it with the input type)\n",
    "        x = nn.GeometricTensor(input, self.input_type)\n",
    "        \n",
    "        # apply each equivariant block\n",
    "        for layer in self.layers:\n",
    "            if \"state\" in inspect.signature(layer).parameters:\n",
    "                x, state = layer(x, state)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "\n",
    "        # unwrap the output GeometricTensor\n",
    "        # (take the Pytorch tensor and discard the associated representation)\n",
    "        x = x.tensor\n",
    "        \n",
    "        # classify with the final fully connected layers)\n",
    "        for layer in self.fully_net:\n",
    "            if \"state\" in inspect.signature(layer).parameters:\n",
    "                x, state = jax.vmap(layer, (0, None), (0, None), axis_name=\"batch\")(x, state)\n",
    "            else:\n",
    "                x = jax.vmap(layer, axis_name=\"batch\")(x)\n",
    "\n",
    "        return x, state\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the model on *rotated* MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: wget: command not found\n",
      "Archive:  mnist_rotation_new.zip\n"
     ]
    }
   ],
   "source": [
    "# download the dataset\n",
    "!wget -nc http://www.iro.umontreal.ca/~lisa/icml2007data/mnist_rotation_new.zip\n",
    "# uncompress the zip file\n",
    "!unzip -n mnist_rotation_new.zip -d mnist_rotation_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import RandomRotation\n",
    "from torchvision.transforms import Pad\n",
    "from torchvision.transforms import Resize\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.transforms import Compose\n",
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistRotDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, mode, transform=None):\n",
    "        assert mode in ['train', 'test']\n",
    "            \n",
    "        if mode == \"train\":\n",
    "            file = \"mnist_rotation_new/mnist_all_rotation_normalized_float_train_valid.amat\"\n",
    "        else:\n",
    "            file = \"mnist_rotation_new/mnist_all_rotation_normalized_float_test.amat\"\n",
    "        \n",
    "        self.transform = transform\n",
    "\n",
    "        data = np.loadtxt(file, delimiter=' ')\n",
    "            \n",
    "        self.images = data[:, :-1].reshape(-1, 28, 28).astype(np.float32)\n",
    "        self.labels = data[:, -1].astype(np.int64)\n",
    "        self.num_samples = len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image, label = self.images[index], self.labels[index]\n",
    "        image = Image.fromarray(image, mode='F')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# images are padded to have shape 29x29.\n",
    "# this allows to use odd-size filters with stride 2 when downsampling a feature map in the model\n",
    "pad = Pad((0, 0, 1, 1), fill=0)\n",
    "\n",
    "# to reduce interpolation artifacts (e.g. when testing the model on rotated images),\n",
    "# we upsample an image by a factor of 3, rotate it and finally downsample it again\n",
    "resize1 = Resize(87)\n",
    "resize2 = Resize(29)\n",
    "\n",
    "totensor = ToTensor()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "model = C8SteerableCNN(key)\n",
    "state = eqx.nn.State(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is now randomly initialized. \n",
    "Therefore, we do not expect it to produce the right class probabilities.\n",
    "\n",
    "However, the model should still produce the same output for rotated versions of the same image.\n",
    "This is true for rotations by multiples of $\\frac{\\pi}{2}$, but is only approximate for rotations by $\\frac{\\pi}{4}$.\n",
    "\n",
    "Let's test it on a random test image:\n",
    "we feed eight rotated versions of the first image in the test set and print the output logits of the model for each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model: eqx.Module, x: Image):\n",
    "    np.set_printoptions(linewidth=10000)\n",
    "    \n",
    "    # evaluate the `model` on 8 rotated versions of the input image `x`\n",
    "    # model = model.eval()\n",
    "    \n",
    "    x = resize1(pad(x))\n",
    "    \n",
    "    print()\n",
    "    print('##########################################################################################')\n",
    "    header = 'angle |  ' + '  '.join([\"{:6d}\".format(d) for d in range(10)])\n",
    "    print(header)\n",
    "    # with torch.no_grad():\n",
    "    for r in range(8):\n",
    "        x_transformed = totensor(resize2(x.rotate(r*45., Image.BILINEAR))).reshape(1, 1, 29, 29)\n",
    "        # print(\"x_transformed\", type(x_transformed), x_transformed)\n",
    "        x_transformed = jnp.array(x_transformed.numpy())#.to(device)\n",
    "\n",
    "        y, _ = model(x_transformed, state)\n",
    "        # y = y.to('cpu').numpy().squeeze()\n",
    "        y = np.array(y).squeeze()\n",
    "        \n",
    "        angle = r * 45\n",
    "        print(\"{:5d} : {}\".format(angle, y))\n",
    "    print('##########################################################################################')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the test set    \n",
    "raw_mnist_test = MnistRotDataset(mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##########################################################################################\n",
      "angle |       0       1       2       3       4       5       6       7       8       9\n",
      "    0 : [-0.11777765  0.00718439 -0.06087103 -0.00774419 -0.07453537  0.05387858 -0.03126591 -0.00302693 -0.06451857 -0.07264927]\n",
      "   45 : [-0.11777765  0.00718439 -0.06087103 -0.00774419 -0.07453537  0.05387858 -0.03126591 -0.00302693 -0.06451857 -0.07264927]\n",
      "   90 : [-0.11777765  0.00718439 -0.06087103 -0.00774419 -0.07453537  0.05387858 -0.03126591 -0.00302693 -0.06451857 -0.07264927]\n",
      "  135 : [-0.11777765  0.00718439 -0.06087103 -0.00774419 -0.07453537  0.05387858 -0.03126591 -0.00302693 -0.06451857 -0.07264927]\n",
      "  180 : [-0.11777765  0.00718439 -0.06087103 -0.00774419 -0.07453537  0.05387858 -0.03126591 -0.00302693 -0.06451857 -0.07264927]\n",
      "  225 : [-0.11777765  0.00718439 -0.06087103 -0.00774419 -0.07453537  0.05387858 -0.03126591 -0.00302693 -0.06451857 -0.07264927]\n",
      "  270 : [-0.11777765  0.00718439 -0.06087103 -0.00774419 -0.07453537  0.05387858 -0.03126591 -0.00302693 -0.06451857 -0.07264927]\n",
      "  315 : [-0.11777765  0.00718439 -0.06087103 -0.00774419 -0.07453537  0.05387858 -0.03126591 -0.00302693 -0.06451857 -0.07264927]\n",
      "##########################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# retrieve the first image from the test set\n",
    "x, y = next(iter(raw_mnist_test))\n",
    "\n",
    "# evaluate the model\n",
    "test_model(model, x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the model is already almost invariant.\n",
    "However, we still observe small fluctuations in the outputs.\n",
    "\n",
    "This is because the model contains some operations which might break equivariance.\n",
    "For instance, every convolution includes a padding of $2$ pixels per side. This is adds information about the actual orientation of the grid where the image/feature map is sampled because the padding is not rotated with the image. \n",
    "\n",
    "During training, the model will observe rotated patterns and will learn to ignore the noise coming from the padding."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's train the model now.\n",
    "The model is exactly the same used to train a normal *PyTorch* architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = Compose([\n",
    "    pad,\n",
    "    resize1,\n",
    "    RandomRotation(180., interpolation=InterpolationMode.BILINEAR, expand=False),\n",
    "    resize2,\n",
    "    totensor,\n",
    "])\n",
    "\n",
    "mnist_train = MnistRotDataset(mode='train', transform=train_transform)\n",
    "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=64)\n",
    "\n",
    "\n",
    "test_transform = Compose([\n",
    "    pad,\n",
    "    totensor,\n",
    "])\n",
    "mnist_test = MnistRotDataset(mode='test', transform=test_transform)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@eqx.filter_jit\n",
    "def loss(\n",
    "    model: nn.EquivariantModule, state: eqx.nn.State, x: Float[Array, \"batch 1 29 29\"], y: Int[Array, \" batch\"]\n",
    ") -> Float[Array, \"\"]:\n",
    "    # Our input has the shape (BATCH_SIZE, 1, 29, 29), but our model operations on\n",
    "    # a single input input image of shape (1, 29, 29).\n",
    "    pred_y, state = model(x, state)\n",
    "    loss = cross_entropy(y, pred_y)\n",
    "    return loss, state\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def loss_wrapper(params, static, *args):\n",
    "    is_param = lambda x: isinstance(x, nn.ParameterArray)\n",
    "    model = eqx.combine(params, static, is_leaf=is_param)\n",
    "    return loss(model, *args)\n",
    "\n",
    "\n",
    "def cross_entropy(\n",
    "    y: Int[Array, \" batch\"], pred_y: Float[Array, \"batch 10\"]\n",
    ") -> Float[Array, \"\"]:\n",
    "    # y are the true targets, and should be integers 0-9.\n",
    "    # pred_y are the logits predictions.\n",
    "    return jnp.mean(optax.softmax_cross_entropy(logits=pred_y, labels=jax.nn.one_hot(y, 10)))\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def compute_accuracy(\n",
    "    model: nn.EquivariantModule, x: Float[Array, \"batch 1 29 29\"], y: Int[Array, \" batch\"]\n",
    ") -> Float[Array, \"\"]:\n",
    "    \"\"\"This function takes as input the current model\n",
    "    and computes the average accuracy on a batch.\n",
    "    \"\"\"\n",
    "    pred_y, _ = model(x)\n",
    "    pred_y = jnp.argmax(pred_y, axis=1)\n",
    "    return jnp.mean(y == pred_y)\n",
    "\n",
    "def evaluate(model: nn.EquivariantModule, state: eqx.nn.State, test_loader: torch.utils.data.DataLoader):\n",
    "    \"\"\"This function evaluates the model on the test dataset,\n",
    "    computing both the average loss and the average accuracy.\n",
    "    \"\"\"\n",
    "    inference_model = model.eval()\n",
    "    inference_model = eqx.Partial(inference_model, state=state)\n",
    "\n",
    "    avg_acc = 0\n",
    "    for x, y in test_loader:\n",
    "        x = jnp.array(x.numpy())\n",
    "        y = jnp.array(y.numpy())\n",
    "        # Note that all the JAX operations happen inside `loss` and `compute_accuracy`,\n",
    "        # and both have JIT wrappers, so this is fast.\n",
    "        avg_acc += compute_accuracy(inference_model, x, y)\n",
    "    return avg_acc / len(test_loader)\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: eqx.Module,\n",
    "    state: eqx.nn.State,\n",
    "    train_loader: torch.utils.data.DataLoader,\n",
    "    test_loader: torch.utils.data.DataLoader,\n",
    "    optim: optax.GradientTransformation,\n",
    "    epochs: int,\n",
    "    print_every: int,\n",
    ") -> eqx.Module:\n",
    "    # Just like earlier: It only makes sense to train the arrays in our model,\n",
    "    # so filter out everything else.\n",
    "    is_param = lambda x: isinstance(x, nn.ParameterArray)\n",
    "    opt_state = optim.init(eqx.filter(model, is_param, is_leaf=is_param))\n",
    "\n",
    "    # Always wrap everything -- computing gradients, running the optimiser, updating\n",
    "    # the model -- into a single JIT region. This ensures things run as fast as\n",
    "    # possible.\n",
    "    @eqx.filter_jit\n",
    "    def make_step(\n",
    "        model: eqx.Module,\n",
    "        state: eqx.nn.State, \n",
    "        opt_state: PyTree,\n",
    "        x: Float[Array, \"batch 1 29 29\"],\n",
    "        y: Int[Array, \" batch\"],\n",
    "    ):\n",
    "        params, static = eqx.partition(model, is_param, is_leaf=is_param)\n",
    "        (loss_value, state), grads = jax.value_and_grad(loss_wrapper, has_aux=True)(params, static, state, x, y)\n",
    "        updates, opt_state = optim.update(grads, opt_state, model)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return model, state, opt_state, loss_value\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        for step, (x, y) in enumerate(train_loader):\n",
    "            # PyTorch dataloaders give PyTorch tensors by default,\n",
    "            # so convert them to NumPy arrays.\n",
    "            x = jnp.array(x.numpy())\n",
    "            y = jnp.array(y.numpy())\n",
    "            model, state, opt_state, train_loss = make_step(model, state, opt_state, x, y)\n",
    "\n",
    "        if (epoch % print_every) == 0 or (epoch == epochs - 1):\n",
    "            test_accuracy = evaluate(model, state, test_loader)\n",
    "            print(\n",
    "                f\"{epoch=}\",\n",
    "                f\"{step=}, train_loss={train_loss.item():.2f}, \"\n",
    "                f\"test_accuracy={100*test_accuracy.item():.1f}%\"\n",
    "            )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0 step=187, train_loss=0.6, test_accuracy=0.9\n",
      "epoch=5 step=187, train_loss=0.7, test_accuracy=0.9\n",
      "epoch=10 step=187, train_loss=0.7, test_accuracy=1.0\n",
      "epoch=15 step=187, train_loss=0.7, test_accuracy=1.0\n",
      "epoch=20 step=187, train_loss=0.7, test_accuracy=1.0\n",
      "epoch=25 step=187, train_loss=0.7, test_accuracy=1.0\n",
      "epoch=30 step=187, train_loss=0.7, test_accuracy=1.0\n"
     ]
    }
   ],
   "source": [
    "optim = optax.adamw(learning_rate=5e-5, weight_decay=1e-5)\n",
    "\n",
    "steps = 31\n",
    "print_every = 5\n",
    "model = train(model, state, train_loader, test_loader, optim, steps, print_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##########################################################################################\n",
      "angle |       0       1       2       3       4       5       6       7       8       9\n",
      "    0 : [-0.11777765  0.00718439 -0.06087103 -0.00774419 -0.07453537  0.05387858 -0.03126591 -0.00302693 -0.06451857 -0.07264927]\n",
      "   45 : [-0.11777765  0.00718439 -0.06087103 -0.00774419 -0.07453537  0.05387858 -0.03126591 -0.00302693 -0.06451857 -0.07264927]\n",
      "   90 : [-0.11777765  0.00718439 -0.06087103 -0.00774419 -0.07453537  0.05387858 -0.03126591 -0.00302693 -0.06451857 -0.07264927]\n",
      "  135 : [-0.11777765  0.00718439 -0.06087103 -0.00774419 -0.07453537  0.05387858 -0.03126591 -0.00302693 -0.06451857 -0.07264927]\n",
      "  180 : [-0.11777765  0.00718439 -0.06087103 -0.00774419 -0.07453537  0.05387858 -0.03126591 -0.00302693 -0.06451857 -0.07264927]\n",
      "  225 : [-0.11777765  0.00718439 -0.06087103 -0.00774419 -0.07453537  0.05387858 -0.03126591 -0.00302693 -0.06451857 -0.07264927]\n",
      "  270 : [-0.11777765  0.00718439 -0.06087103 -0.00774419 -0.07453537  0.05387858 -0.03126591 -0.00302693 -0.06451857 -0.07264927]\n",
      "  315 : [-0.11777765  0.00718439 -0.06087103 -0.00774419 -0.07453537  0.05387858 -0.03126591 -0.00302693 -0.06451857 -0.07264927]\n",
      "##########################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "# retrieve the first image from the test set\n",
    "x, y = next(iter(raw_mnist_test))\n",
    "\n",
    "\n",
    "# evaluate the model\n",
    "test_model(model, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
