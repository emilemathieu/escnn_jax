{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# E(n)-Equivariant Steerable CNNs  -  Equivariant MLPs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-26 11:52:51.933151: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-26 11:52:51.968346: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-26 11:52:52.559299: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from escnn_jax import gspaces\n",
    "from escnn_jax import nn\n",
    "from escnn_jax import group\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from typing import List, Tuple, Any, Mapping\n",
    "from jaxtyping import Array, Float, Int, PyTree, PRNGKeyArray\n",
    "import equinox as eqx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **escnn** library also supports MLPs equivariant to compact groups, which can be seen as a special case for $n=0$.\n",
    "This is done by replacing the convolution layers (e.g. [R3Conv](https://quva-lab.github.io/escnn/api/escnn.nn.html#r3conv)) with the [Linear](https://quva-lab.github.io/escnn/api/escnn.nn.html#linear) layer and by choosing the [no_base_space](https://quva-lab.github.io/escnn/api/escnn.gspaces.html#group-action-trivial-on-single-point) `GSpace` (e.g., instead of [rot3dOnR3](https://quva-lab.github.io/escnn/api/escnn.gspaces.html#escnn.gspaces.rot3dOnR3)). \n",
    "\n",
    "All other modules can be used in a similar way, e.g. batch-norm and non-linearities.\n",
    "\n",
    "\n",
    "Here, we provide an example with `G=SO(3)` and one with `G=O(2)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sphere_grid = \"thomson\"\n",
    "sphere_grid = 'ico'\n",
    "\n",
    "class SO3MLP(nn.EquivariantModule):\n",
    "    G: group.Group\n",
    "    gspace: gspaces.GSpace\n",
    "    layers: List\n",
    "    \n",
    "    def __init__(self, key: PRNGKeyArray, n_classes=10):\n",
    "        keys = jax.random.split(key, 8)\n",
    "\n",
    "        super(SO3MLP, self).__init__()\n",
    "        \n",
    "        # the model is equivariant to the group SO(3)\n",
    "        self.G = group.so3_group()\n",
    "        \n",
    "        # since we are building an MLP, there is no base-space\n",
    "        self.gspace = gspaces.no_base_space(self.G)\n",
    "        \n",
    "        # the input contains the coordinates of a point in the 3D space\n",
    "        self.in_type = self.gspace.type(self.G.standard_representation())\n",
    "        \n",
    "        # Layer 1\n",
    "        # We will use the representation of SO(3) acting on signals over a sphere, bandlimited to frequency 1\n",
    "        # To apply a point-wise non-linearity (e.g. ELU), we need to sample the spherical signals over a finite number of points.\n",
    "        # Note that this makes the equivariance only approximate.\n",
    "        # The representation of SO(3) on spherical signals is technically a quotient representation,\n",
    "        # identified by the subgroup of planar rotations, which has id=(False, -1) in our library\n",
    "        \n",
    "        # N.B.: the first this model is instantiated, the library computes numerically the spherical grids, which can take some time\n",
    "        # These grids are then cached on disk, so future calls should be considerably faster.\n",
    "        \n",
    "        activation1 = nn.QuotientFourierELU(\n",
    "            self.gspace,\n",
    "            subgroup_id=(False, -1),\n",
    "            channels=3, # specify the number of spherical signals in the output features\n",
    "            irreps=self.G.bl_sphere_representation(L=1).irreps, # include all frequencies up to L=1\n",
    "            grid=self.G.sphere_grid(type=sphere_grid, N=16), # build a discretization of the sphere containing 16 equally distributed points            \n",
    "            # inplace=True\n",
    "        )\n",
    "        \n",
    "        # map with an equivariant Linear layer to the input expected by the activation function, apply batchnorm and finally the activation\n",
    "        block1 = nn.SequentialModule(\n",
    "            nn.Linear(self.in_type, activation1.in_type, key=keys[0]),\n",
    "            # nn.IIDBatchNorm1d(activation1.in_type),\n",
    "            # activation1,\n",
    "        )\n",
    "        \n",
    "        # Repeat a similar process for a few layers\n",
    "        \n",
    "        # 8 spherical signals, bandlimited up to frequency 3\n",
    "        activation2 = nn.QuotientFourierELU(\n",
    "            self.gspace,\n",
    "            subgroup_id=(False, -1),\n",
    "            channels=8, # specify the number of spherical signals in the output features\n",
    "            irreps=self.G.bl_sphere_representation(L=3).irreps, # include all frequencies up to L=3\n",
    "            grid=self.G.sphere_grid(type=sphere_grid, N=40), # build a discretization of the sphere containing 40 equally distributed points            \n",
    "            # inplace=True\n",
    "        )\n",
    "        block2 = nn.SequentialModule(\n",
    "            nn.Linear(block1.out_type, activation2.in_type, key=keys[1]),\n",
    "            # nn.IIDBatchNorm1d(activation2.in_type),\n",
    "            activation2,\n",
    "        )\n",
    "        \n",
    "        # 8 spherical signals, bandlimited up to frequency 3\n",
    "        activation3 = nn.QuotientFourierELU(\n",
    "            self.gspace,\n",
    "            subgroup_id=(False, -1),\n",
    "            channels=8, # specify the number of spherical signals in the output features\n",
    "            irreps=self.G.bl_sphere_representation(L=3).irreps, # include all frequencies up to L=3\n",
    "            grid=self.G.sphere_grid(type=sphere_grid, N=40), # build a discretization of the sphere containing 40 equally distributed points            \n",
    "            # inplace=True\n",
    "        )\n",
    "        block3 = nn.SequentialModule(\n",
    "            nn.Linear(block2.out_type, activation3.in_type, key=keys[2]),\n",
    "            # nn.IIDBatchNorm1d(activation3.in_type),\n",
    "            activation3,\n",
    "        )\n",
    "        \n",
    "        # 5 spherical signals, bandlimited up to frequency 2\n",
    "        activation4 = nn.QuotientFourierELU(\n",
    "            self.gspace,\n",
    "            subgroup_id=(False, -1),\n",
    "            channels=5, # specify the number of spherical signals in the output features\n",
    "            irreps=self.G.bl_sphere_representation(L=2).irreps, # include all frequencies up to L=2\n",
    "            grid=self.G.sphere_grid(type=sphere_grid, N=25), # build a discretization of the sphere containing 25 equally distributed points            \n",
    "            # inplace=True\n",
    "        )\n",
    "        block4 = nn.SequentialModule(\n",
    "            nn.Linear(block3.out_type, activation4.in_type, key=keys[3]),\n",
    "            # nn.IIDBatchNorm1d(activation4.in_type),\n",
    "            activation4,\n",
    "        )\n",
    "        \n",
    "        # Final linear layer mapping to the output features\n",
    "        # the output is a 5-dimensional vector transforming according to the Wigner-D matrix of frequency 2\n",
    "        self.out_type = self.gspace.type(self.G.irrep(2))\n",
    "        block5 = nn.Linear(block4.out_type, self.out_type, key=keys[4])\n",
    "\n",
    "        self.layers = [block1, block2, block3, block4, block5]\n",
    "    \n",
    "    def __call__(self, x: nn.GeometricTensor):\n",
    "        # check the input has the right type\n",
    "        assert x.type == self.in_type\n",
    "        \n",
    "        # apply each equivariant block\n",
    "        \n",
    "        # Each layer has an input and an output type\n",
    "        # A layer takes a GeometricTensor in input.\n",
    "        # This tensor needs to be associated with the same representation of the layer's input type\n",
    "        #\n",
    "        # The Layer outputs a new GeometricTensor, associated with the layer's output type.\n",
    "        # As a result, consecutive layers need to have matching input/output types\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "     \n",
    "        return x\n",
    "    \n",
    "    def evaluate_output_shape(self, input_shape: tuple):\n",
    "        shape = list(input_shape)\n",
    "        assert len(shape) ==2, shape\n",
    "        assert shape[1] == self.in_type.size, shape\n",
    "        shape[1] = self.out_type.size\n",
    "        return shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 5678\n",
    "key = jax.random.PRNGKey(SEED)\n",
    "model = SO3MLP(key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the equivariance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################################################################################\n",
      "Outputs' magnitudes\n",
      "[0.1641 0.0205 0.1317 0.0536 0.1148 0.0538 0.1501 0.1078 0.1102 0.1649]\n",
      "##########################################################################################\n",
      "Errors' magnitudes\n",
      "[0.0066 0.0004 0.0016 0.0014 0.002  0.0009 0.0038 0.0018 0.002  0.0077]\n",
      "[0.0086 0.0004 0.0022 0.0014 0.0018 0.0014 0.0018 0.0026 0.0028 0.0082]\n",
      "[0.0051 0.0004 0.0025 0.0007 0.0026 0.0015 0.0016 0.002  0.0029 0.0071]\n",
      "[0.0077 0.0001 0.001  0.0009 0.0016 0.0003 0.0036 0.0022 0.0005 0.0024]\n",
      "[0.009  0.0002 0.0023 0.0012 0.0031 0.0013 0.0046 0.0016 0.0029 0.0068]\n",
      "[0.0066 0.0003 0.0017 0.0007 0.0013 0.0003 0.0034 0.0018 0.0019 0.0025]\n",
      "[0.0032 0.0004 0.0023 0.0014 0.0025 0.0015 0.0045 0.001  0.0029 0.0091]\n",
      "[0.0083 0.0002 0.0017 0.0014 0.0024 0.0014 0.0041 0.0027 0.003  0.0076]\n",
      "##########################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(linewidth=10000, precision=4, suppress=True)\n",
    "\n",
    "model = model.eval()\n",
    "\n",
    "B = 10\n",
    "\n",
    "# generates B random points in 3D and wrap them in a GeometricTensor of the right type\n",
    "x = jax.lax.stop_gradient(jax.random.normal(key, (B, 3)))\n",
    "x = model.in_type(x)\n",
    "\n",
    "print('##########################################################################################')\n",
    "y = model(x)\n",
    "print(\"Outputs' magnitudes\")\n",
    "print(jnp.linalg.norm(y.tensor, axis=1).reshape(-1))\n",
    "print('##########################################################################################')\n",
    "print(\"Errors' magnitudes\")\n",
    "for r in range(8):\n",
    "    # sample a random rotation\n",
    "    g = model.G.sample()\n",
    "    \n",
    "    x_transformed = g @ x\n",
    "    x_transformed = x_transformed #.to(device)\n",
    "\n",
    "    y_transformed = model(x_transformed) #.to('cpu')\n",
    "    \n",
    "    # verify that f(g@x) = g@f(x)=g@y\n",
    "    print(jnp.linalg.norm(y_transformed.tensor - (g@y).tensor, axis=1).reshape(-1))        \n",
    "\n",
    "print('##########################################################################################')\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SO3MLPtensor(nn.EquivariantModule):\n",
    "    G: group.Group\n",
    "    gspace: gspaces.GSpace\n",
    "    layers: List\n",
    "    \n",
    "    def __init__(self, key: PRNGKeyArray, n_classes=10):\n",
    "        keys = jax.random.split(key, 8)\n",
    "        \n",
    "        super(SO3MLPtensor, self).__init__()\n",
    "        \n",
    "        # the model is equivariant to the group SO(3)\n",
    "        self.G = group.so3_group()\n",
    "        \n",
    "        # since we are building an MLP, there is no base-space\n",
    "        self.gspace = gspaces.no_base_space(self.G)\n",
    "        \n",
    "        # the input contains the coordinates of a point in the 3D space\n",
    "        in_repr = self.G.standard_representation()\n",
    "        self.in_type = self.gspace.type(in_repr)\n",
    "        \n",
    "        # Layer 1\n",
    "        # We will use the representation of SO(3) acting on signals over a sphere, bandlimited to frequency 2\n",
    "        # We use the tensor-product non-linearity, which is essentially a quadratic function.\n",
    "        \n",
    "        ttype = self.gspace.type(self.G.bl_sphere_representation(L=2))\n",
    "        activation1 = nn.TensorProductModule(self.in_type, ttype, key=keys[0])\n",
    "        \n",
    "        # First we apply batch-norm and then the non-linearity. \n",
    "        # In the next blocks, we will also include a Linear layer.\n",
    "        block1 = nn.SequentialModule(\n",
    "            # nn.IIDBatchNorm1d(activation1.in_type),\n",
    "            activation1,\n",
    "        )\n",
    "        \n",
    "        # Repeat a similar process for a few layers\n",
    "        \n",
    "        # input and output types must have the same number of fields (here, 8)\n",
    "        # the input one shouldn't have frequencies higher than the output of the previous block\n",
    "        activation2 = nn.TensorProductModule(\n",
    "            in_type = self.gspace.type(*[self.G.bl_sphere_representation(L=2)]*8),\n",
    "            out_type = self.gspace.type(*[self.G.bl_sphere_representation(L=3)]*8),\n",
    "            key=keys[1]    \n",
    "        )\n",
    "        block2 = nn.SequentialModule(\n",
    "            nn.Linear(block1.out_type, activation2.in_type, key=keys[2]),\n",
    "            # nn.IIDBatchNorm1d(activation2.in_type),\n",
    "            activation2,\n",
    "        )\n",
    "        \n",
    "        activation3 = nn.TensorProductModule(\n",
    "            in_type = self.gspace.type(*[self.G.bl_sphere_representation(L=3)]*8),\n",
    "            out_type = self.gspace.type(*[self.G.bl_sphere_representation(L=3)]*8),\n",
    "            key=keys[3]   \n",
    "        )\n",
    "        block3 = nn.SequentialModule(\n",
    "            nn.Linear(block2.out_type, activation3.in_type, key=keys[4]),\n",
    "            # nn.IIDBatchNorm1d(activation3.in_type),\n",
    "            activation3,\n",
    "        )\n",
    "        \n",
    "        activation4 = nn.TensorProductModule(\n",
    "            in_type = self.gspace.type(*[self.G.bl_sphere_representation(L=3)]*8),\n",
    "            out_type = self.gspace.type(*[self.G.irrep(2)]*8),    # the final layer only require frequency 2 features, so there is no point in generating other frequencies\n",
    "            key=keys[5]\n",
    "        )\n",
    "        block4 = nn.SequentialModule(\n",
    "            nn.Linear(block3.out_type, activation4.in_type, key=keys[6]),\n",
    "            # nn.IIDBatchNorm1d(activation4.in_type),\n",
    "            activation4,\n",
    "        )\n",
    "        \n",
    "        # Final linear layer mapping to the output features\n",
    "        # the output is a 5-dimensional vector transforming according to the Wigner-D matrix of frequency 2\n",
    "        self.out_type = self.gspace.type(self.G.irrep(2))\n",
    "        block5 = nn.Linear(block4.out_type, self.out_type, key=keys[7])\n",
    "        self.layers = [block1, block2, block3, block4, block5]\n",
    "    \n",
    "    def __call__(self, x: nn.GeometricTensor):\n",
    "        \n",
    "        # check the input has the right type\n",
    "        assert x.type == self.in_type\n",
    "        \n",
    "        # apply each equivariant block\n",
    "        \n",
    "        # Each layer has an input and an output type\n",
    "        # A layer takes a GeometricTensor in input.\n",
    "        # This tensor needs to be associated with the same representation of the layer's input type\n",
    "        #\n",
    "        # The Layer outputs a new GeometricTensor, associated with the layer's output type.\n",
    "        # As a result, consecutive layers need to have matching input/output types\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "     \n",
    "        return x\n",
    "\n",
    "    def evaluate_output_shape(self, input_shape: tuple):\n",
    "        shape = list(input_shape)\n",
    "        assert len(shape) ==2, shape\n",
    "        assert shape[1] == self.in_type.size, shape\n",
    "        shape[1] = self.out_type.size\n",
    "        return shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 5678\n",
    "key = jax.random.PRNGKey(SEED)\n",
    "model = SO3MLPtensor(key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the equivariance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################################################################################\n",
      "Outputs' magnitudes\n",
      "[0.1713 0.1712 0.0006 2.4142 0.     0.3532]\n",
      "##########################################################################################\n",
      "Errors' magnitudes\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "##########################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(linewidth=10000, precision=4, suppress=True)\n",
    "\n",
    "model = model.eval()\n",
    "\n",
    "B = 6\n",
    "\n",
    "# generates B random points in 3D and wrap them in a GeometricTensor of the right type\n",
    "x = jax.lax.stop_gradient(jax.random.normal(key, (B, 3)))\n",
    "x = model.in_type(x)\n",
    "\n",
    "print('##########################################################################################')\n",
    "y = model(x)\n",
    "print(\"Outputs' magnitudes\")\n",
    "print(jnp.linalg.norm(y.tensor, axis=1).reshape(-1))\n",
    "print('##########################################################################################')\n",
    "print(\"Errors' magnitudes\")\n",
    "for r in range(8):\n",
    "    # sample a random rotation\n",
    "    g = model.G.sample()\n",
    "    \n",
    "    x_transformed = g @ x\n",
    "    x_transformed = x_transformed\n",
    "\n",
    "    y_transformed = model(x_transformed)\n",
    "    \n",
    "    # verify that f(g@x) = g@f(x)=g@y\n",
    "    print(jnp.linalg.norm(y_transformed.tensor - (g@y).tensor, axis=1).reshape(-1))        \n",
    "\n",
    "print('##########################################################################################')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SO2MLP(nn.EquivariantModule):\n",
    "    G: group.Group\n",
    "    gspace: gspaces.GSpace\n",
    "    layers: List\n",
    "    \n",
    "    def __init__(self, key: PRNGKeyArray, n_classes=10):\n",
    "        keys = jax.random.split(key, 8)\n",
    "        \n",
    "        super(SO2MLP, self).__init__()\n",
    "        \n",
    "        # the model is equivariant to the group O(2)\n",
    "        self.G = group.so2_group()\n",
    "        \n",
    "        # since we are building an MLP, there is no base-space\n",
    "        self.gspace = gspaces.no_base_space(self.G)\n",
    "        \n",
    "        # the input contains the coordinates of a point in the 2D space\n",
    "        self.in_type = self.gspace.type(self.G.standard_representation())\n",
    "        \n",
    "        # Layer 1\n",
    "        # We will use the regular representation of SO(2) acting on signals over SO(2) itself, bandlimited to frequency 1\n",
    "        # Most of the comments on the previous SO(3) network apply here as well\n",
    "       \n",
    "        activation1 = nn.FourierELU(\n",
    "            self.gspace,\n",
    "            channels=3, # specify the number of signals in the output features\n",
    "            irreps=self.G.bl_regular_representation(L=1).irreps, # include all frequencies up to L=1\n",
    "            # inplace=True,\n",
    "            # the following kwargs are used to build a discretization of the circle containing 6 equally distributed points\n",
    "            type='regular', N=6,   \n",
    "        )\n",
    "        \n",
    "        # map with an equivariant Linear layer to the input expected by the activation function, apply batchnorm and finally the activation\n",
    "        block1 = nn.SequentialModule(\n",
    "            nn.Linear(self.in_type, activation1.in_type, key=keys[0]),\n",
    "            # nn.IIDBatchNorm1d(activation1.in_type),\n",
    "            activation1,\n",
    "        )\n",
    "        \n",
    "        # Repeat a similar process for a few layers\n",
    "        \n",
    "        # 8 signals, bandlimited up to frequency 3\n",
    "        activation2 = nn.FourierELU(\n",
    "            self.gspace,\n",
    "            channels=8, # specify the number of signals in the output features\n",
    "            irreps=self.G.bl_regular_representation(L=3).irreps, # include all frequencies up to L=3\n",
    "            # inplace=True,\n",
    "            # the following kwargs are used to build a discretization of the circle containing 16 equally distributed points\n",
    "            type='regular', N=16,\n",
    "        )\n",
    "        block2 = nn.SequentialModule(\n",
    "            nn.Linear(block1.out_type, activation2.in_type, key=keys[1]),\n",
    "            # nn.IIDBatchNorm1d(activation2.in_type),\n",
    "            activation2,\n",
    "        )\n",
    "        \n",
    "        # 8 signals, bandlimited up to frequency 3\n",
    "        activation3 = nn.FourierELU(\n",
    "            self.gspace,\n",
    "            channels=8, # specify the number of signals in the output features\n",
    "            irreps=self.G.bl_regular_representation(L=3).irreps, # include all frequencies up to L=3\n",
    "            # inplace=True,\n",
    "            # the following kwargs are used to build a discretization of the circle containing 16 equally distributed points\n",
    "            type='regular', N=16,\n",
    "        )\n",
    "        block3 = nn.SequentialModule(\n",
    "            nn.Linear(block2.out_type, activation3.in_type, key=keys[2]),\n",
    "            # nn.IIDBatchNorm1d(activation3.in_type),\n",
    "            activation3,\n",
    "        )\n",
    "        \n",
    "        # 5 signals, bandlimited up to frequency 2\n",
    "        activation4 = nn.FourierELU(\n",
    "            self.gspace,\n",
    "            channels=5, # specify the number of signals in the output features\n",
    "            irreps=self.G.bl_regular_representation(L=2).irreps, # include all frequencies up to L=2\n",
    "            # inplace=True,\n",
    "            # the following kwargs are used to build a discretization of the circle containing 12 equally distributed points\n",
    "            type='regular', N=12,\n",
    "        )\n",
    "        block4 = nn.SequentialModule(\n",
    "            nn.Linear(block3.out_type, activation4.in_type, key=keys[3]),\n",
    "            # nn.IIDBatchNorm1d(activation4.in_type),\n",
    "            activation4,\n",
    "        )\n",
    "        \n",
    "        # Final linear layer mapping to the output features\n",
    "        # the output is a 2-dimensional vector rotating with frequency 2\n",
    "        self.out_type = self.gspace.type(self.G.irrep(2))\n",
    "        block5 = nn.Linear(block4.out_type, self.out_type, key=keys[4])\n",
    "\n",
    "        self.layers = [block1, block2, block3, block4, block5]\n",
    "    \n",
    "    def __call__(self, x: nn.GeometricTensor):\n",
    "        \n",
    "        # check the input has the right type\n",
    "        assert x.type == self.in_type\n",
    "        \n",
    "        # apply each equivariant block\n",
    "        \n",
    "        # Each layer has an input and an output type\n",
    "        # A layer takes a GeometricTensor in input.\n",
    "        # This tensor needs to be associated with the same representation of the layer's input type\n",
    "        #\n",
    "        # The Layer outputs a new GeometricTensor, associated with the layer's output type.\n",
    "        # As a result, consecutive layers need to have matching input/output types\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "     \n",
    "        return x\n",
    "    \n",
    "    def evaluate_output_shape(self, input_shape: tuple):\n",
    "        shape = list(input_shape)\n",
    "        assert len(shape) ==2, shape\n",
    "        assert shape[1] == self.in_type.size, shape\n",
    "        shape[1] = self.out_type.size\n",
    "        return shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 5678\n",
    "key = jax.random.PRNGKey(SEED)\n",
    "model = SO2MLP(key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the equivariance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################################################################################\n",
      "Outputs' magnitudes\n",
      "[0.0454 0.0744 0.0052 0.0691 0.1648 0.1498]\n",
      "##########################################################################################\n",
      "Errors' magnitudes\n",
      "[0.     0.0006 0.     0.0003 0.0015 0.001 ]\n",
      "[0.0001 0.0003 0.     0.0003 0.0008 0.0011]\n",
      "[0.0004 0.0011 0.     0.0003 0.0056 0.0033]\n",
      "[0.0005 0.0012 0.     0.0006 0.0066 0.0037]\n",
      "[0.0004 0.0011 0.     0.0004 0.0054 0.0029]\n",
      "[0.0005 0.0013 0.     0.0004 0.0067 0.0039]\n",
      "[0.0003 0.0012 0.     0.0003 0.0052 0.0031]\n",
      "[0.0005 0.0012 0.     0.0007 0.0066 0.0031]\n",
      "##########################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(linewidth=10000, precision=4, suppress=True)\n",
    "\n",
    "model = model.eval()\n",
    "\n",
    "B = 6\n",
    "\n",
    "# generates B random points in 3D and wrap them in a GeometricTensor of the right type\n",
    "x = jax.lax.stop_gradient(jax.random.normal(key, (B, 2)))\n",
    "x = model.in_type(x)\n",
    "\n",
    "print('##########################################################################################')\n",
    "y = model(x)\n",
    "print(\"Outputs' magnitudes\")\n",
    "print(jnp.linalg.norm(y.tensor, axis=1).reshape(-1))\n",
    "print('##########################################################################################')\n",
    "print(\"Errors' magnitudes\")\n",
    "for r in range(8):\n",
    "    # sample a random rotation\n",
    "    g = model.G.sample()\n",
    "    \n",
    "    x_transformed = g @ x\n",
    "    x_transformed = x_transformed\n",
    "\n",
    "    y_transformed = model(x_transformed)\n",
    "    \n",
    "    # verify that f(g@x) = g@f(x)=g@y\n",
    "    print(jnp.linalg.norm(y_transformed.tensor - (g@y).tensor, axis=1).reshape(-1))        \n",
    "\n",
    "print('##########################################################################################')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
